<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Vida Adeli</title>

    <meta name="author" content="Vida Adeli">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Vida Adeli
                </p>
                <p style="margin-bottom: 20px;">

                  I'm a Computer Science PhD candidate at University of Toronto, supervised by <a href="https://www.cs.toronto.edu/~taati/">Dr. Babak Taati</a> and <a href="https://rsi.utoronto.ca/faculty/andrea-iaboni">Dr. Andrea Iaboni</a>. I am also a Faculty Affiliate Researcher at <a href="https://vectorinstitute.ai/">Vector Institute</a> and a Research Assistant at <a href="https://kite-uhn.com/">KITE - University Health Network</a>. Previously, I was a Researcher at <a href="https://vl4ai.erc.monash.edu/">Vision & Learning for Autonomous AI (VL4AI) Lab</a>, Monash University. I'm currently doing an internship at <a href="https://www.linkedin.com/company/pickford-ai/posts/?feedView=all">Pickford AI</a>, focusing on video generation and real-time 3D animation generation.
                </p>
                <!-- <p style="text-align:center">
                  <a href="">Email</a> &nbsp;/&nbsp;
                  <a href="">CV</a> &nbsp;/&nbsp;
                  <a href="">Bio</a> &nbsp;/&nbsp;
                  <a href="">Scholar</a> &nbsp;/&nbsp;
                  <a href="">Twitter</a> &nbsp;/&nbsp;
                  <a href="">Github</a>
                </p> -->

                <div class="social-links" style="margin-bottom: 30px;">
                  <a href="https://www.linkedin.com/in/vida-adeli/" target="_blank">
                    <img src="/assets/images/linkedin.png" alt="LinkedIn" class="social-icon">
                  </a>
                  <a href="https://scholar.google.com/citations?user=j_mCCb0AAAAJ&hl=en" target="_blank">
                    <img src="/assets/images/scholar-b.png" alt="Google Scholar" class="social-icon">
                  </a>
                  <a href="https://github.com/vadeli" target="_blank">
                    <img src="/assets/images/github.png" alt="GitHub" class="social-icon">
                  </a>
                  <a href="https://x.com/vida_adl" target="_blank">
                    <img src="/assets/images/twitter.png" alt="Twitter" class="social-icon">
                  </a>
                  <a href="mailto:vida.adeli@mail.utoronto.ca">
                    <img src="/assets/images/email.png" alt="Email" class="social-icon">
                  </a>
                  <a href="./assets/vida_adeli_resume.pdf">
                    <img src="/assets/images/cv2.png" alt="Email" class="social-icon">
                  </a>
                </div>

              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="assets/images/VidaAdeli.png"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="assets/images/VidaAdeli.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  <!-- My research interests broadly span Computer Vision and Deep Learning. My research focuses on generative models and their applications across a range of computer vision tasks.  I am interested in 3D human motion analysis for animation generation and also  applications in healthcare.  -->
                   My research focuses on generative models applied across a range of computer vision tasks, with particular focus on 3D human motion understanding, realistic animation generation, and clinical gait analysis.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>



      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Selected publications</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


      <tr>
        <td style="padding:20px;width:30%;max-width:30%" align="center">
          <video style="width:100%;max-width:100%" autoplay muted loop playsinline>
            <source src="assets/videos/pickstyle.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://pickstyle.pickford.ai/">
            <span class="papertitle">PickStyle: Video-to-Video Style Transfer with Context-Style Adapters</span>
          </a>
          <br>
          <a href="https://soroushmehraban.github.io/">Soroush Mehraban*</a>,
          <strong>Vida Adeli*</strong>,
          <a href="">Jacob Rommann</a>,
          <a href="https://www.cs.toronto.edu/~taati/">Babak Taati</a>
          <a href="https://kyrylai.com/">Kyryl Truskovskyi</a>
          <br>
          <em>* Equal contribution</em><br>
          <em>arXiv</em>
          <br>
          <a href="https://pickstyle.pickford.ai/">Project page</a>
          /
          <a href="https://arxiv.org/abs/2510.07546">arXiv</a>
          <p></p>
          <p>
            PickStyle is a diffusion-based video style transfer framework that preserves video context while applying a target visual style. It uses low-rank style adapters and synthetic clip augmentation from paired images for training, and introduces Context-Style Classifier-Free Guidance (CS-CFG) to independently control content and style, achieving temporally consistent and style-faithful video results.
          </p>
        </td>
      </tr>

      <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="assets/images/carepd.png" alt="dise">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2510.04312">
              <span class="papertitle">CARE-PD: A Multi-Site Anonymized Clinical Dataset for Parkinson's Disease Gait Assessment</span>
            </a>
            <br>
            <strong>Vida Adeli</strong>,
            <a href="https://soroushmehraban.github.io/">Soroush Mehraban</a>,
            <a>Ivan Klabucar</a>,
            <a href="https://www.cs.toronto.edu/~rajabi/">Javad Rajabi</a>,
            <a href="https://www.tudelft.nl/en/staff/b.filtjens/">Benjamin Filtjens</a>,
            <a href="https://soroushmehraban.github.io/">Soroush Mehraban</a>,
            <a>et al.</a>
            <br>
            <em>Neural Information Processing Systems (NeurIPS)</em>, 2025
            <br>
            <a href="https://neurips2025.care-pd.ca/">Project page</a>
            /
            <a href="https://arxiv.org/abs/2510.04312">arXiv</a>
            /
            <a href="https://borealisdata.ca/dataset.xhtml?persistentId=doi:10.5683/SP3/TWIKMK">Dataset</a>
            <p></p>
            <p>
              CARE-PD is the largest publicly available archive of 3D mesh gait data for Parkinsonâ€™s Disease, collected across 9 cohorts from 8 clinical centers. It provides standardized, anonymized SMPL representations and benchmark protocols for clinical motion analysis on PD.
            </p>
          </td>
        </tr>


      <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <video style="width:100%;max-width:100%" autoplay muted loop playsinline>
              <source src="assets/videos/gaitgen.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://vadeli.github.io/GAITGen/">
              <span class="papertitle">GAITGen: Disentangled Motion-Pathology Impaired Gait Generative Model</span>
            </a>
            <br>
            <strong>Vida Adeli</strong>,
            <a href="https://soroushmehraban.github.io/">Soroush Mehraban</a>,
            <a href="https://majidmirmehdi.github.io/">Majid Mirmehdi</a>,
            <a href="https://research-information.bris.ac.uk/en/persons/alan-l-whone">Alan Whone</a>,
            <a href="https://www.tudelft.nl/en/staff/b.filtjens/">Benjamin Filtjens</a>,
            <a href="https://research-information.bris.ac.uk/en/persons/amirhossein-dadashzadeh-2">Amirhossein Dadashzadeh</a>,
            <a href="https://www.uhnresearch.ca/researcher/alfonso-fasano">Alfonso Fasano</a>,
            <a href="https://psychiatry.utoronto.ca/faculty/andrea-iaboni">Andrea Iaboni</a>,
            <a href="https://www.cs.toronto.edu/~taati/">Babak Taati</a>
            <br>
            <em>arXiv</em>
            <br>
            <a href="https://vadeli.github.io/GAITGen/">project page</a>
            /
            <a href="https://arxiv.org/abs/2503.22397">arXiv</a>
            <p></p>
            <p>
              GAITGen is a generative framework that synthesizes realistic gait sequences conditioned on Parkinsonâ€™s severity. Using a Conditional Residual VQ-VAE and tailored Transformers, it disentangles motion and pathology features to produce clinically meaningful gait data. GAITGen enhances dataset diversity and improves performance in parkinsonian gait analysis tasks.
            </p>
          </td>
        </tr>


        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="assets/images/benchmark.png" alt="dise">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://github.com/TaatiTeam/MotionEncoders_parkinsonism_benchmark">
              <span class="papertitle">Benchmarking Skeleton-based Motion Encoder Models for Clinical
                Applications: Estimating Parkinson's Disease Severity in Walking Sequences</span>
            </a>
            <br>
            <strong>Vida Adeli</strong>,
            <a href="https://soroushmehraban.github.io/">Soroush Mehraban</a>,
            <a href="https://cvl.tuwien.ac.at/staff/irene-ballester-campos/">Irene Ballester</a>,
            <a href="https://scholar.google.com/citations?user=NIi-VHAAAAAJ&hl=en">Yasamin Zarghami</a>,
            <a href="https://scholar.google.ca/citations?user=KxhRl2UAAAAJ&hl=en">Andrea Sabo</a>,
            <a href="https://www.uhnresearch.ca/researcher/andrea-iaboni">Andrea Iaboni</a>,
            <a href="https://www.cs.toronto.edu/~taati/">Babak Taati</a>
            <br>
            <em>IEEE International Conference on Automatic Face and Gesture Recognition (FG)</em>, 2024
            <br>
            <a href="https://github.com/TaatiTeam/MotionEncoders_parkinsonism_benchmark">Code</a>
            /
            <a href="https://arxiv.org/abs/2405.17817">arXiv</a>
            <p></p>
            <p>
              Evaluating recent motion encoders for the task of parkinsonism severity estimation (UPDRS III gait)
            </p>
          </td>
        </tr>

        
        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="assets/images/motionagformer.gif" alt="dise">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://github.com/TaatiTeam/MotionAGFormer">
                <span class="papertitle">MotionAGFormer: Enhancing 3D Pose Estimation with a Transformer-GCNFormer
                  Network</span>
              </a>
              <br>
              <a href="https://soroushmehraban.github.io/">Soroush Mehraban</a>,
              <strong>Vida Adeli</strong>,
              <a href="https://www.cs.toronto.edu/~taati/">Babak Taati</a>
              <br>
              <em>Winter Conference on Applications of Computer Vision (WACV)</em>, 2024
              <br>
              <a href="https://github.com/TaatiTeam/MotionAGFormer">Code</a>
              /
              <a href="https://www.youtube.com/watch?v=iyLhxPjwBuQ">video</a>
              /
              <a href="https://arxiv.org/abs/2310.16288">arXiv</a>
              <p></p>
              <p>
                Estimating 3D locations of 17 main joints from a monocular video.
              </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="assets/images/Fall.png" alt="dise">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/10102699">
                <span class="papertitle">Ambient Monitoring of Gait and Machine Learning Models for Dynamic and Short-Term Falls Risk Assessment in People With Dementia</span>
              </a>
              <br>
              <strong>Vida Adeli</strong>,
              <a href="https://soroushmehraban.github.io/">Soroush Mehraban</a>,
              <a>Navid Korhani</a>,
              <a href="https://scholar.google.ca/citations?user=KxhRl2UAAAAJ&hl=en">Andrea Sabo</a>,
              <a href="https://scholar.google.com/citations?user=cZqeRc0AAAAJ&hl=en">Sina Mehdizadeh</a>,
              <a href="https://psychiatry.utoronto.ca/faculty/alastair-flint">Alastair Flint</a>,
              <a href="https://rsi.utoronto.ca/faculty/avril-mansfield">Avril Mansfield</a>,
              <a href="https://www.uhnresearch.ca/researcher/andrea-iaboni">Andrea Iaboni</a>,
              <a href="https://www.cs.toronto.edu/~taati/">Babak Taati</a>
              <br>
              <em>IEEE journal of biomedical and health informatics (JBHI)</em>, 2023
              <br>
              <a href="https://www.techrxiv.org/doi/full/10.36227/techrxiv.16943395.v2">arXiv</a>
              <p></p>
              <p>
                A machine learning system using ambient gait monitoring to predict short-term fall risk in people with dementia.
              </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="assets/images/iccv2021_tripod.png" alt="dise">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Adeli_TRiPOD_Human_Trajectory_and_Pose_Dynamics_Forecasting_in_the_Wild_ICCV_2021_paper.pdf">
                <span class="papertitle">TRiPOD: Human Trajectory and Pose Dynamics Forecasting in the Wild</span>
              </a>
              <br>
              <strong>Vida Adeli</strong>,
              <a>Mahsa Ehsanpour</a>,
              <a href="https://scholar.google.com/citations?user=ATkNLcQAAAAJ&hl=en">Ian Reid</a>,
              <a href="https://www.niebles.net/">Juan Carlos Niebles</a>,
              <a href="https://scholar.google.com/citations?user=ImpbxLsAAAAJ&hl=en">Silvio Savarese</a>,
              <a href="https://stanford.edu/~eadeli/">Ehsan Adeli</a>,
              <a href="https://vl4ai.erc.monash.edu/">Hamid Rezatofighi</a>
              <br>
              <em>IEEE/CVF International Conference on Computer Vision</em>, (ICCV 2021)
              <br>
              <a href="https://somof.stanford.edu/">Project Page</a> /
              <a href="https://arxiv.org/pdf/2104.04029">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=4SNWjCWUYgA">Video</a> /
              <a href="https://somof.stanford.edu/">Benchmark</a> /
              <a href="https://somof.stanford.edu/workshops/iccv21">ICCV2021 Workshop</a>
              <p></p>
              <p>
                TRiPOD jointly forecasts human body pose dynamics and global trajectories in the wild by modelling human-human and human-object interactions.
              </p>
            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="assets/images/Iros2020.png" alt="dise">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9145701">
                <span class="papertitle">Socially and Contextually Aware Human Motion and Pose Forecasting</span>
              </a>
              <br>
              <strong>Vida Adeli</strong>,
              <a href="https://stanford.edu/~eadeli/">Ehsan Adeli</a>,
              <a href="https://scholar.google.com/citations?user=ATkNLcQAAAAJ&hl=en">Ian Reid</a>,
              <a href="https://www.niebles.net/">Juan Carlos Niebles</a>,
              <a href="https://vl4ai.erc.monash.edu/">Hamid Rezatofighi</a>
              <br>
              <em>IEEE Robotics and Automation Letters (RA-L) and IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2020
              <br>
              <a href="https://arxiv.org/pdf/2007.06843">arXiv</a>/
              <a href="https://www.youtube.com/watch?v=sTBNdBco1Yo">Video</a>
              <p></p>
              <p>
                A unified end-to-end model that predicts both human global motion (trajectory) and detailed body pose jointly, using social and scene context to improve forecasting.
              </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="assets/images/imagecomputing.png" alt="dise">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.sciencedirect.com/science/article/pii/S0262885619301246">
                <span class="papertitle">A component-based video content representation for action recognition</span>
              </a>
              <br>
              <strong>Vida Adeli</strong>,
              Ehsan Fazl-Ersi,
              Ahad Harati,
              <br>
              <em>Image and Vision Computing</em>, 2019
              <br>
              <a href="https://www.sciencedirect.com/science/article/abs/pii/S0262885619301246">Paper</a>
              <p></p>
              <p>
                Proposes a component-based video content representation for human action recognition, decomposing videos into meaningful parts to improve classification in complex scenes.
              </p>
            </td>
          </tr>


          <!-- <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="assets/images/imagecomputing.png" alt="dise">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.sciencedirect.com/science/article/pii/S0262885619301246">
                <span class="papertitle">A component-based video content representation for action recognition</span>
              </a>
              <br>
              <strong>Vida Adeli</strong>,
              Ehsan Fazl-Ersi,
              Ahad Harati,
              <br>
              <em>International Conference on Pattern Recognition and Artificial Intelligence (ICPRAI)</em>, 2018
              <br>
              <a href="">Paper</a>
              <p></p>
              <p>
                Proposes a component-based video content representation for human action recognition, decomposing videos into meaningful parts to improve classification in complex scenes.
              </p>
            </td>
          </tr> -->

          </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Benchmarks and Workshops</h2>
                <ul>
                  <li><strong><a href="https://neurips2025.care-pd.ca/">CARE-PD</a></strong> Benchmark</li>
                  <li><strong>SoMoF</strong>: Created SOcial MOtion Forecasting Benchmark (<a href="https://somof.stanford.edu/">SoMoF</a>), a key resource for human motion forecasting research.</li>
                  <li><strong><a href="https://somof.stanford.edu/workshops/iccv21">ICCV2021 Workshop</a></strong>, Workshop and Challenge on Human Trajectory and Pose Dynamics Forecasting in the Wild.</li>
                </ul>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Honors and Awards</h2>
                <ul>
                  <li>Best Presentation Award, AI and Demenia International Workshop, AGE-WELL (2025)</li>
                  <li>Beatrice "Trixie" Worsley Graduate Scholarship In Computer Science (2025)</li>
                  <li><strong>Outstanding Reviewer</strong>, IEEE Conference on Computer Vision and Pattern Recognition (CVPR2022)</li>
                  <li>Mount Sinai Hospital Graduate Scholarship in Science and Technology (2022)</li>
                </ul>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


        <table style="width:100%;padding-top:30px;margin-right:auto;margin-left:auto;">
          <tbody>
              <tr>
              <td style="width:100%;vertical-align:middle">
                <h2>Teaching Assistant</h2>
                <ul>
                  <li>Introduction to Image Understanding (<a href="https://www.cs.toronto.edu/~lindell/teaching/420/2025/">CSC420</a>)</li>
                  <li>Foundations of Computational Vision (<a href="https://sgs.calendar.utoronto.ca/course/csc2503h">CSC2503</a>)</li>
                  <li>Introduction to Visual Computing (<a href="https://artsci.calendar.utoronto.ca/course/csc320h1">CSC320</a>)</li>
                  <li>Applied Fundamentals of Deep Learning (<a href="https://engineering.calendar.utoronto.ca/course/aps360h1">APS360</a>)</li>
                </ul>
              </td>
            </tr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Academic Service</h2>
                <ul>
                  <li>Member of Reviewing Committee: CVPR, ICCV, ECCV, IROS, JBHI, ICLR, WACV, NeurIPS</li>
                </ul>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          </tbody></table>
          


          </tbody></table>

          <table
        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tr>
          <td style="padding:0px">
            <br>
            <p style="text-align:center;font-size:small;">Website Template <a style="font-size:small;"
                href="https://github.com/jonbarron/jonbarron_website">source code</a>.
            </p>
          </td>
        </tr>
      </table>

        </td>
      </tr>
    </table>
  </body>
</html>
